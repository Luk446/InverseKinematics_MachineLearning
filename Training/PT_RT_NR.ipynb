{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreReqs\n",
    "\n",
    "Import essential libraries, split into three chunks \n",
    "- machine learning packaages\n",
    "- tuning for auto-hyperparameter selection\n",
    "- linear algerba, data-manipulation and miscellanious QOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installation - python version must be 3.12 or below due to comptability with some modules\n",
    "# \"type : ignore\" used to stop error reporting of non resolve (not relevant)\n",
    "\n",
    "# ML from PyTorch\n",
    "import torch # type: ignore\n",
    "import torch.optim as optim # type: ignore\n",
    "import torch.nn.functional as F # type: ignore\n",
    "from torch import nn # type: ignore\n",
    "from torch.utils.data import Dataset, DataLoader # type: ignore\n",
    "from torch.utils.data import random_split  # type: ignore\n",
    "from torchvision.transforms import ToTensor # type: ignore\n",
    "from torch.utils.data import TensorDataset, DataLoader # type: ignore\n",
    "import torchvision # type: ignore\n",
    "import torchvision.transforms as transforms # type: ignore\n",
    "from torchmetrics.regression import MeanAbsolutePercentageError  # type: ignore\n",
    "\n",
    "# Tuning from Ray \n",
    "import ray # type: ignore\n",
    "from ray import tune  # type: ignore\n",
    "from ray import train  # type: ignore\n",
    "from ray.tune import CLIReporter  # type: ignore\n",
    "from ray.tune.schedulers import ASHAScheduler  # type: ignore\n",
    "from ray.train import Checkpoint, get_checkpoint  # type: ignore\n",
    "from ray.tune.schedulers import ASHAScheduler  # type: ignore\n",
    "import ray.cloudpickle as pickle  # type: ignore\n",
    "\n",
    "\n",
    "# Linear algebra, array manip and data analysis\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "from mpl_toolkits.mplot3d import Axes3D # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.preprocessing import StandardScaler # type: ignore\n",
    "\n",
    "# misc\n",
    "from functools import partial\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndevice = (\\n    \"cuda\"\\n    if torch.cuda.is_available()\\n    else \"mps\"\\n    if torch.backends.mps.is_available()\\n    else \"cpu\"\\n)\\nprint(f\"Using {device} device\")\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Block used if neccesary for trouble shooting to check whether CUDA is working\n",
    "\n",
    "\"\"\"\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import, conversion and preparation \n",
    "\n",
    "- importing the data from the csv and previewing \n",
    "- extrapolate the data into targets (I) and inputs (C)\n",
    "- split the data chunks into training testing and validation\n",
    "    -    The data is split into training (80%), validation (20% of the remaining 80%), and test sets using train_test_split\n",
    "- The data is then normalised with StandardScaler and converted into pytorch tensors (now compatible with learning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>roll</th>\n",
       "      <th>pitch</th>\n",
       "      <th>yaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1170</td>\n",
       "      <td>732</td>\n",
       "      <td>1609</td>\n",
       "      <td>-0.444512</td>\n",
       "      <td>-0.326699</td>\n",
       "      <td>-0.119629</td>\n",
       "      <td>125.507812</td>\n",
       "      <td>66.115723</td>\n",
       "      <td>-20.544434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2099</td>\n",
       "      <td>1990</td>\n",
       "      <td>2400</td>\n",
       "      <td>-0.303887</td>\n",
       "      <td>-0.247598</td>\n",
       "      <td>-0.849121</td>\n",
       "      <td>114.916992</td>\n",
       "      <td>78.112793</td>\n",
       "      <td>-23.005371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>65</td>\n",
       "      <td>549</td>\n",
       "      <td>795</td>\n",
       "      <td>-0.044609</td>\n",
       "      <td>0.121543</td>\n",
       "      <td>-0.080078</td>\n",
       "      <td>74.860840</td>\n",
       "      <td>85.759277</td>\n",
       "      <td>-71.696777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2493</td>\n",
       "      <td>403</td>\n",
       "      <td>56</td>\n",
       "      <td>0.535469</td>\n",
       "      <td>-1.214395</td>\n",
       "      <td>0.394531</td>\n",
       "      <td>-159.653320</td>\n",
       "      <td>32.036133</td>\n",
       "      <td>45.021973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>967</td>\n",
       "      <td>2464</td>\n",
       "      <td>523</td>\n",
       "      <td>1.506660</td>\n",
       "      <td>0.723594</td>\n",
       "      <td>0.284668</td>\n",
       "      <td>-68.686523</td>\n",
       "      <td>19.445801</td>\n",
       "      <td>137.834473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1    p2    p3         x         y         z        roll      pitch  \\\n",
       "0  1170   732  1609 -0.444512 -0.326699 -0.119629  125.507812  66.115723   \n",
       "1  2099  1990  2400 -0.303887 -0.247598 -0.849121  114.916992  78.112793   \n",
       "2    65   549   795 -0.044609  0.121543 -0.080078   74.860840  85.759277   \n",
       "3  2493   403    56  0.535469 -1.214395  0.394531 -159.653320  32.036133   \n",
       "4   967  2464   523  1.506660  0.723594  0.284668  -68.686523  19.445801   \n",
       "\n",
       "          yaw  \n",
       "0  -20.544434  \n",
       "1  -23.005371  \n",
       "2  -71.696777  \n",
       "3   45.021973  \n",
       "4  137.834473  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas data import \n",
    "data = pd.read_csv('All_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrap data and create arrays\n",
    "I = data[['x','y','z']].values\n",
    "C = data[['p1','p2','p3']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_train: (10842, 3), C_train: (10842, 3)\n",
      "I_test: (3615, 3), C_test: (3615, 3)\n",
      "I_val: (3615, 3), C_val: (3615, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- split the data into training and testing sets\n",
    "- 80% of the data will be used for training, 20% for testing\n",
    "- the random state is set to 30 to ensure the same result each time the function is called\n",
    "- the data is split into 4 arrays: I_train, I_test, C_train, C_test\n",
    "- I_train and C_train will be used to train the model\n",
    "- I_test and C_test will be used to test the model\n",
    "- the shape of the data is printed to confirm the split\n",
    "\"\"\"\n",
    "\n",
    "# splits the data into 80% training and 20% testing for both I and C\n",
    "I_train,I_test,C_train,C_test=train_test_split(I,C,test_size=0.2,random_state=30)\n",
    "# arrays (I,C) , split size (20%) , int in random state allows for same result each time func is called \n",
    "\n",
    "# splits the data again , out of the 80 (training) take another 20 for validation \n",
    "I_train,I_val,C_train,C_val=train_test_split(I_train,C_train,test_size=0.25,random_state=30)\n",
    "# arrays (I and C train) , 25% , rand. int\n",
    "\n",
    "# print the shape of the data to confirm the splits\n",
    "print(f\"I_train: {I_train.shape}, C_train: {C_train.shape}\")\n",
    "print(f\"I_test: {I_test.shape}, C_test: {C_test.shape}\")\n",
    "print(f\"I_val: {I_val.shape}, C_val: {C_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Normalise the input features and target values\n",
    "- Fit the scalers on the training data and transform all splits\n",
    "- Convert to PyTorch tensors\n",
    "\"\"\"\n",
    "\n",
    "# Normalise the input features and target values\n",
    "scaler_input = StandardScaler()\n",
    "scaler_output = StandardScaler()\n",
    "\n",
    "\n",
    "# Fit the scalers on the training data and transform all splits\n",
    "I_train = scaler_input.fit_transform(I_train)\n",
    "I_val = scaler_input.transform(I_val)\n",
    "I_test = scaler_input.transform(I_test)\n",
    "\n",
    "C_train = scaler_output.fit_transform(C_train)\n",
    "C_val = scaler_output.transform(C_val)\n",
    "C_test = scaler_output.transform(C_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "I_train = torch.tensor(I_train, dtype=torch.float32)\n",
    "I_val = torch.tensor(I_val, dtype=torch.float32)\n",
    "I_test = torch.tensor(I_test, dtype=torch.float32)\n",
    "\n",
    "C_train = torch.tensor(C_train, dtype=torch.float32)\n",
    "C_val = torch.tensor(C_val, dtype=torch.float32)\n",
    "C_test = torch.tensor(C_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model section\n",
    "\n",
    "- a multi-layer precepteron regressor class is defined\n",
    "    - 3 points on input and output layer - to match data (x,y,z) -> (p1,p2,p3)\n",
    "    - l1, l2 and l3 hidden layer configurations\n",
    "    - dropout layers between each to prevent over fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Define the MLPRegressor class\n",
    "- The class inherits from nn.Module\n",
    "- The class has a constructor that takes the following parameters:\n",
    "    - l1: number of neurons in the first hidden layer\n",
    "    - l2: number of neurons in the second hidden layer\n",
    "    - l3: number of neurons in the third hidden layer\n",
    "    - activation: activation function to use in the hidden layers\n",
    "    - dropout_prob: dropout probability\n",
    "\"\"\"\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "\n",
    "    # Define the model architecture\n",
    "    def __init__(self, l1=120, l2=84, l3=10, activation='Tanh', dropout_prob=0.2): # set defaults here \n",
    "        # Call the parent class constructor\n",
    "        super(MLPRegressor, self).__init__()\n",
    "\n",
    "        # Dynamically choose the activation function based on config\n",
    "        self.activation = getattr(nn, activation.capitalize(), nn.Tanh)() \n",
    "\n",
    "        # Define hidden layer params \n",
    "        self.hidden_layers = nn.Sequential(\n",
    "            nn.Linear(3, l1), # input layer -> hidden\n",
    "            self.activation, # acitviation function configured in search space\n",
    "            nn.Dropout(p=dropout_prob), # drop out layer - probability config in search space\n",
    "            nn.Linear(l1, l2), # hidden 1 -> 2\n",
    "            self.activation,\n",
    "            nn.Dropout(p=dropout_prob),\n",
    "            nn.Linear(l2, l3), # hidden 2 -> 3\n",
    "            self.activation,\n",
    "            nn.Dropout(p=dropout_prob)\n",
    "        )\n",
    "        \n",
    "        # Define output layer with 3 outputs\n",
    "        self.output_layer = nn.Linear(l3, 3)  # hidden 3 -> output\n",
    "        \n",
    "    # Define the forward pass    \n",
    "    def forward(self, x): # x is the input data\n",
    "        x = self.hidden_layers(x) # pass through hidden layers\n",
    "        return self.output_layer(x) # pass through output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function\n",
    "\n",
    "- The train function accepts a configuration dictionary for hyperparameters (l1, l2) and trains the MLPRegressor model.\n",
    "- Loss is computed with nn.MSELoss, and the model is optimized using NAdam. The script also handles checkpointing with Ray Tune.\n",
    "- Validation Metrics: The script calculates validation loss and Mean Absolute Percentage Error (MAPE) for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config, data_dir=None, max_epochs=1000):\n",
    "\n",
    "    # Initialize the checkpoint directory outside of any conditionals\n",
    "    checkpoint_dir = \"./checkpoints\"  # This ensures it's defined before use\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True) # create the directory if it doesn't exist\n",
    "\n",
    "    \"\"\"\n",
    "    Preliminary section\n",
    "     - deining the mode\n",
    "     - device allocation\n",
    "     - selecting criterion and optimiser\n",
    "     - creation of the data loaders for train and eval\n",
    "     - checkpointing logic\n",
    "    \"\"\"\n",
    "    \n",
    "    # define model with variables configurable in the mlpr class def \n",
    "    model = MLPRegressor(\n",
    "        l1=config[\"l1\"],\n",
    "        l2=config[\"l2\"],\n",
    "        l3=config[\"l3\"],\n",
    "        activation=config[\"activation\"],\n",
    "        dropout_prob=config[\"dropout_prob\"]\n",
    "    )\n",
    "    \n",
    "    # use the cuda device if available\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # check for parralel computing ability \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = nn.DataParallel(model)\n",
    "    # assign the model to the chosen device\n",
    "    model.to(device)\n",
    "    \n",
    "    # set for regression using mean squared error loss\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Each optimiser has its own variables so they are defined here by checking which one is being used\n",
    "    if config[\"optimiser\"] == \"NAdam\":\n",
    "        optimiser = optim.NAdam(model.parameters(), lr=config[\"learning_rate_init\"], betas=(config[\"momentum\"], 0.999))\n",
    "    elif config[\"optimiser\"] == \"AdamW\":\n",
    "        optimiser = optim.AdamW(model.parameters(), lr=config[\"learning_rate_init\"], betas=(config[\"momentum\"], 0.999), weight_decay=1e-4)\n",
    "    elif config[\"optimiser\"] == \"RAdam\":\n",
    "        optimiser = optim.RAdam(model.parameters(), lr=config[\"learning_rate_init\"], betas=(config[\"momentum\"], 0.999))\n",
    "    elif config[\"optimiser\"] == \"SGD\":\n",
    "        optimiser = optim.SGD(model.parameters(), lr=config[\"learning_rate_init\"], momentum=config[\"momentum\"], weight_decay=1e-4)\n",
    "    elif config[\"optimiser\"] == \"Adam\":\n",
    "        optimiser = optim.Adam(model.parameters(), lr=config[\"learning_rate_init\"], betas=(config[\"momentum\"], 0.999))\n",
    "\n",
    "\n",
    "    # For batch sizing to be available we need to use data loading \n",
    "\n",
    "    # Training dataset\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(I_train, C_train),\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True\n",
    "    )\n",
    "    # Validation dataset\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(I_val, C_val),\n",
    "        batch_size=config[\"batch_size\"]\n",
    "    )\n",
    "\n",
    "    # fetch the checkpoint for resuming the trial\n",
    "    checkpoint = get_checkpoint() # get the checkpoint from the trial\n",
    "    if checkpoint: # if there is a checkpoint\n",
    "        with checkpoint.as_directory() as checkpoint_dir: # open the directory\n",
    "            data_path = Path(checkpoint_dir) / \"data.pkl\" # get the data path\n",
    "            with open(data_path, \"rb\") as fp:           # open the file\n",
    "                checkpoint_state = pickle.load(fp)      # load the checkpoint state\n",
    "            start_epoch = checkpoint_state[\"epoch\"]     # get the epoch from the checkpoint\n",
    "            model.load_state_dict(checkpoint_state[\"model_state_dict\"]) # load the model state\n",
    "            optimiser.load_state_dict(checkpoint_state[\"optimizer_state_dict\"]) # load the optimiser state\n",
    "    else: # if there is no checkpoint\n",
    "        start_epoch = 0 # start the epoch at 0\n",
    "    \n",
    "    \"\"\"\n",
    "    Training and evaluation loops \n",
    "    - loops over a max defined range in the function definition \n",
    "    - train first \n",
    "    - check results using eval\n",
    "    \"\"\"\n",
    "\n",
    "    for epoch in range(start_epoch, max_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Train section\n",
    "        - put into training mode\n",
    "        - init the variables used to track the training progress\n",
    "        - run  forwards and backwards over the model\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        # epoch_steps = 0\n",
    "\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "\n",
    "        \"\"\"\n",
    "        Eval section\n",
    "        - put into eval mode\n",
    "        - init tracking variables\n",
    "        - execute model validation using val set\n",
    "        \"\"\"\n",
    "\n",
    "        # eval mode\n",
    "        model.eval()\n",
    "\n",
    "        # Validation variable inits\n",
    "        val_loss = 0.0\n",
    "        mape_total = 0.0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, targets in val_loader:\n",
    "\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                # Calculate loss\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "                # Calculate MAPE\n",
    "                mapecalc = MeanAbsolutePercentageError().to(device)\n",
    "                mape_total += mapecalc(outputs, targets).item()\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        Tracking section \n",
    "        - track the running average of the stats\n",
    "        - keep a checkpoint of the trail\n",
    "        - report stats into the ray tune progress reporter\n",
    "        \"\"\"\n",
    "\n",
    "        # track the average of the eval statistics \n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_mape = mape_total / len(val_loader)\n",
    "\n",
    "        # keep a checkpoint of the trails as they run \n",
    "        checkpoint_data = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimiser.state_dict(),\n",
    "        }\n",
    "        # checkpoint saving\n",
    "          # Create a temporary directory for each checkpoint\n",
    "        with tempfile.TemporaryDirectory() as checkpoint_dir:\n",
    "            data_path = os.path.join(checkpoint_dir, \"data.pkl\")\n",
    "            with open(data_path, \"wb\") as fp:\n",
    "                pickle.dump(checkpoint_data, fp)\n",
    "\n",
    "            # Provide the checkpoint to Ray\n",
    "            checkpoint = Checkpoint.from_directory(checkpoint_dir)\n",
    "\n",
    "            # report the stats to the ray tune progress reporter\n",
    "            ray.train.report(\n",
    "                {\"loss\": avg_val_loss, \"mape\": avg_mape},\n",
    "                checkpoint=checkpoint\n",
    "            )\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparmater tuning with ray tune \n",
    "\n",
    "The main function is the core of the tuning\n",
    "- sets up Ray Tune to optimize hyperparameters\n",
    "    - activation\n",
    "    - alpha\n",
    "    - layer sizes\n",
    "    - learning rate\n",
    "    - iterations\n",
    "    - tol\n",
    "    - momentum\n",
    "    - val frac\n",
    "- An ASHA Scheduler (ASHAScheduler) is used to terminate underperforming trials early.\n",
    "- Resource Allocation: The script dynamically assigns CPU and GPU resources based on availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_samples=1000, max_num_epochs=10):\n",
    "\n",
    "    \"\"\"\n",
    "    - Allocate resources based on the available resources\n",
    "    - Define the hyper-parameter search space\n",
    "    - Define the early stopping scheduler\n",
    "    - Define the checkpoint directory\n",
    "    - Run the hyper-parameter search\n",
    "    - Load the best checkpoint\n",
    "    - Save the best trained model\n",
    "    \"\"\"\n",
    "\n",
    "    # Allocate based on available resources\n",
    "    resources = ray.cluster_resources()\n",
    "    available_cpus = int(resources.get(\"CPU\", 0))\n",
    "    available_gpus = int(resources.get(\"GPU\", 0))\n",
    "\n",
    "    resources_per_trial = {\n",
    "        \"cpu\": max(1, available_cpus // num_samples),\n",
    "        \"gpu\": max(1, available_gpus) if available_gpus > 0 else 0\n",
    "    }\n",
    "\n",
    "    # hyper-parameter search area \n",
    "    config = {\n",
    "        \"activation\": tune.choice(['tanh', 'RReLU', 'Hardtanh', 'identity','LeakyReLU']),\n",
    "        \"alpha\": tune.loguniform(1e-5, 1e-2),\n",
    "        \"batch_size\": tune.choice([8, 32, 128, 256]),\n",
    "        \"l1\": tune.choice([2**i for i in range(12)]),\n",
    "        \"l2\": tune.choice([2**i for i in range(12)]),\n",
    "        \"l3\": tune.choice([2**i for i in range(11)]),\n",
    "        \"learning_rate_init\": tune.loguniform(1e-6, 1e-1),\n",
    "        \"max_iter\": tune.choice([4000,10000,15000,20000]),\n",
    "        \"tol\": tune.loguniform(1e-4, 1e-3),\n",
    "        \"momentum\": tune.uniform(0.5, 0.99),\n",
    "        \"validation_fraction\": tune.uniform(0.1, 0.3),\n",
    "        \"dropout_prob\": tune.uniform(0.1,0.3),\n",
    "        \"optimiser\": tune.choice(['NAdam', 'AdamW', 'RAdam', 'SGD', 'Adam'])\n",
    "    }\n",
    "    \n",
    "    # early stopping for trials with bad mape values\n",
    "    scheduler = ASHAScheduler(\n",
    "        metric=\"mape\",\n",
    "        mode=\"min\",\n",
    "        max_t=max_num_epochs,\n",
    "        grace_period=10,\n",
    "        reduction_factor=2,\n",
    "    )\n",
    "\n",
    "    # checkpoint bugfix for names being too long\n",
    "    def short_dirname(trial):\n",
    "        return \"trial_\" + str(trial.trial_id)\n",
    "\n",
    "    # send values into the progress reporter in the output \n",
    "    result = tune.run(\n",
    "        partial(train, max_epochs=max_num_epochs),\n",
    "        resources_per_trial=resources_per_trial,\n",
    "        config=config,\n",
    "        num_samples=num_samples,\n",
    "        scheduler=scheduler,\n",
    "        trial_dirname_creator=short_dirname\n",
    "    )\n",
    "\n",
    "    # get best trial \n",
    "    best_trial = result.get_best_trial(\"mape\", \"min\", \"last\")\n",
    "    print(f\"Best trial config: {best_trial.config}\")\n",
    "    print(f\"Best trial final mape: {best_trial.last_result['mape']}\")\n",
    "\n",
    "\n",
    "    # Define the best trained model with the same hyper parameters decided in the config space \n",
    "    best_trained_model = MLPRegressor(\n",
    "        l1=best_trial.config[\"l1\"],\n",
    "        l2=best_trial.config[\"l2\"],\n",
    "        l3=best_trial.config[\"l3\"],\n",
    "        activation=best_trial.config[\"activation\"],\n",
    "        dropout_prob=best_trial.config[\"dropout_prob\"],\n",
    "        )\n",
    "\n",
    "    # Device selection \n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        best_trained_model = nn.DataParallel(best_trained_model)\n",
    "    best_trained_model.to(device)\n",
    "\n",
    "\n",
    "    # Load the best checkpoint\n",
    "    best_checkpoint = result.get_best_checkpoint(best_trial, metric=\"mape\", mode=\"min\")\n",
    "    if best_checkpoint:\n",
    "        with best_checkpoint.as_directory() as checkpoint_dir:\n",
    "            checkpoint_path = Path(checkpoint_dir) / \"data.pkl\"\n",
    "            with open(checkpoint_path, \"rb\") as fp:\n",
    "                checkpoint_state = pickle.load(fp)\n",
    "            best_trained_model.load_state_dict(checkpoint_state[\"model_state_dict\"])\n",
    "        print(f\"Best checkpoint loaded from: {best_checkpoint}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found for the best trial.\")\n",
    "\n",
    "    # Save the best trained model\n",
    "    torch.save(best_trained_model,\"PR_RT_NR_T27.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray init function\n",
    "if not ray.is_initialized():\n",
    "        ray.init()\n",
    "# main function call\n",
    "if __name__ == \"__main__\":\n",
    "    # check for ray init\n",
    "    if not ray.is_initialized():\n",
    "        ray.init()\n",
    "\n",
    "    try:\n",
    "        main()\n",
    "    # catch the error if the file is not found\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FileNotFoundError: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
